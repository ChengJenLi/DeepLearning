{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 匯入Keras及SVG等函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 匯入IMDB資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 匯入Numpy等工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical, np_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料數，特徵數，向量維度，Step數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = 5000\n",
    "valid_reviews = 100\n",
    "max_features = 5000\n",
    "embedding_size = 256\n",
    "step_size = 5\n",
    "batch_size = 32\n",
    "index_from = 2\n",
    "rnn_units = 128\n",
    "epochs = 2\n",
    "word_index_prev = {'<PAD>': 0, '<START>': 1, '<UNK>': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀入IMDB資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 13s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, index_from=index_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽取IMDB中的單字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = {word: (index + index_from) for word, index in imdb.get_word_index().items() if (index + index_from) < max_features}\n",
    "word_index.update(word_index_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以單字資訊製作字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {index: word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示文章函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence(sentence):\n",
    "    for index in sentence:\n",
    "        print(index_word[index], end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示第一篇文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all \n"
     ]
    }
   ],
   "source": [
    "print_sentence(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將資料分為學習以及測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [t for s in x_train[:train_reviews] for t in s]\n",
    "data_valid = [t for s in x_train[train_reviews:train_reviews+valid_reviews] for t in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批次處理的函式定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size, step_size):\n",
    "    seg_len = len(data) // batch_size\n",
    "    steps_per_epoch = seg_len // step_size\n",
    "    data_seg_list = np.asarray([data[int(i*seg_len):int((i+1)*seg_len)] for i in range(batch_size)])\n",
    "    data_seg_list\n",
    "    i = 0\n",
    "    while True:\n",
    "        x = data_seg_list[:, int(i*step_size):int((i+1)*step_size)]\n",
    "        y = np.asarray([to_categorical(data_seg_list[j, int(i*step_size+1):int((i+1)*step_size+1)], max_features) for j in range(batch_size)])\n",
    "        yield x, y\n",
    "        i += 1\n",
    "        if i >= steps_per_epoch:\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用LSTM的設計模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Input(shape=(step_size,), name='Input')\n",
    "x = Embedding(input_dim=max_features, output_dim=embedding_size, name='Embedding')(w)\n",
    "y = LSTM(units=rnn_units, return_sequences=True, dropout=0.5, recurrent_dropout=0.5, name='LSTM')(x)\n",
    "w_next = TimeDistributed(Dense(units=max_features, activation='softmax', name='Dense'), name='TimeDistributed')(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 製作模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[w], outputs=[w_next])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"296pt\" viewBox=\"0.00 0.00 464.19 296.00\" width=\"464pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 292)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-292 460.1875,-292 460.1875,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140326142319696 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140326142319696</title>\n",
       "<polygon fill=\"none\" points=\"107.8677,-243.5 107.8677,-287.5 348.3198,-287.5 348.3198,-243.5 107.8677,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165.4351\" y=\"-261.3\">Input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"223.0024,-243.5 223.0024,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250.8369\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"223.0024,-265.5 278.6714,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"250.8369\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"278.6714,-243.5 278.6714,-287.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313.4956\" y=\"-272.3\">(None, 5)</text>\n",
       "<polyline fill=\"none\" points=\"278.6714,-265.5 348.3198,-265.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313.4956\" y=\"-250.3\">(None, 5)</text>\n",
       "</g>\n",
       "<!-- 140326142317792 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140326142317792</title>\n",
       "<polygon fill=\"none\" points=\"75.1953,-162.5 75.1953,-206.5 380.9922,-206.5 380.9922,-162.5 75.1953,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.4351\" y=\"-180.3\">Embedding: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"227.6748,-162.5 227.6748,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.5093\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"227.6748,-184.5 283.3438,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.5093\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"283.3438,-162.5 283.3438,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332.168\" y=\"-191.3\">(None, 5)</text>\n",
       "<polyline fill=\"none\" points=\"283.3438,-184.5 380.9922,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332.168\" y=\"-169.3\">(None, 5, 256)</text>\n",
       "</g>\n",
       "<!-- 140326142319696&#45;&gt;140326142317792 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140326142319696-&gt;140326142317792</title>\n",
       "<path d=\"M228.0938,-243.3664C228.0938,-235.1516 228.0938,-225.6579 228.0938,-216.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"231.5938,-216.6068 228.0938,-206.6068 224.5938,-216.6069 231.5938,-216.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140326142317400 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140326142317400</title>\n",
       "<polygon fill=\"none\" points=\"102.4023,-81.5 102.4023,-125.5 353.7852,-125.5 353.7852,-81.5 102.4023,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.4351\" y=\"-99.3\">LSTM: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"200.4678,-81.5 200.4678,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"228.3022\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"200.4678,-103.5 256.1367,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"228.3022\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"256.1367,-81.5 256.1367,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304.9609\" y=\"-110.3\">(None, 5, 256)</text>\n",
       "<polyline fill=\"none\" points=\"256.1367,-103.5 353.7852,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304.9609\" y=\"-88.3\">(None, 5, 128)</text>\n",
       "</g>\n",
       "<!-- 140326142317792&#45;&gt;140326142317400 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140326142317792-&gt;140326142317400</title>\n",
       "<path d=\"M228.0938,-162.3664C228.0938,-154.1516 228.0938,-144.6579 228.0938,-135.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"231.5938,-135.6068 228.0938,-125.6068 224.5938,-135.6069 231.5938,-135.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140326073620296 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140326073620296</title>\n",
       "<polygon fill=\"none\" points=\"0,-.5 0,-44.5 456.1875,-44.5 456.1875,-.5 0,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"147.9351\" y=\"-18.3\">TimeDistributed(Dense): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"295.8701,-.5 295.8701,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.7046\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"295.8701,-22.5 351.5391,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.7046\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"351.5391,-.5 351.5391,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"403.8633\" y=\"-29.3\">(None, 5, 128)</text>\n",
       "<polyline fill=\"none\" points=\"351.5391,-22.5 456.1875,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"403.8633\" y=\"-7.3\">(None, 5, 5000)</text>\n",
       "</g>\n",
       "<!-- 140326142317400&#45;&gt;140326073620296 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140326142317400-&gt;140326073620296</title>\n",
       "<path d=\"M228.0938,-81.3664C228.0938,-73.1516 228.0938,-63.6579 228.0938,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"231.5938,-54.6068 228.0938,-44.6068 224.5938,-54.6069 231.5938,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 編譯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 製作批量處理的資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train = batch_generator(data_train, batch_size, step_size)\n",
    "gen_valid = batch_generator(data_valid, batch_size, step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算Step數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_train = len(data_train) / batch_size / step_size\n",
    "steps_per_epoch_valid = len(data_valid) / batch_size / step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批次逐次對資料學習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7620/7619 [==============================] - 267s 35ms/step - loss: 5.4899 - acc: 0.1719 - val_loss: 5.4978 - val_acc: 0.1709\n",
      "Epoch 2/2\n",
      "7620/7619 [==============================] - 261s 34ms/step - loss: 5.6126 - acc: 0.1731 - val_loss: 5.5772 - val_acc: 0.1714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0384e7748>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=gen_train, steps_per_epoch=steps_per_epoch_train, epochs=epochs,\n",
    "                    validation_data=gen_valid, validation_steps=steps_per_epoch_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑選接著出現單字的函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.log(preds) / temperature\n",
    "    preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "    choices = range(len(preds))\n",
    "    return np.random.choice(choices, p=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隨機生成文章的函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sentences(num_sentences, sample_sent_len = 20):\n",
    "    for x_test_i in x_test[:num_sentences]:\n",
    "        x = np.zeros((1, step_size))\n",
    "        sentence = x_test_i[:step_size]\n",
    "\n",
    "        for i in range(sample_sent_len):\n",
    "            for j, index in enumerate(sentence[-step_size:]):\n",
    "                x[0, j] = index\n",
    "            preds = model.predict(x)[0][-1]\n",
    "            next_index = sample(preds)\n",
    "            sentence.append(next_index)\n",
    "\n",
    "        print_sentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隨機抽出文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> please give this one <UNK> it actually makes it having him the first time just though they were at \n",
      "<START> this film requires a movie <UNK> who <UNK> <UNK> are foster this was 8 then stop the town in \n",
      "<START> many animation buffs consider it i'm some of name their own horror wonderful movie turns on your hands of \n",
      "<START> i generally love this film is a third <UNK> br br overall i be enjoyed walk the original overall \n",
      "<START> like some other people are a john easily <UNK> <UNK> one of the fact how many being <UNK> and \n",
      "<START> i'm absolutely <UNK> this is his character for imdb to play if their talent they worse other scenes are \n",
      "<START> originally supposed to be better to watch the film particularly made before a comments into mind someone here the \n",
      "<START> the <UNK> richard <UNK> as i get up with a little <UNK> about the <UNK> of this movie i \n",
      "<START> hollywood had a long guy who kills someone we are the character went on the entertaining <UNK> and <UNK> \n",
      "<START> this film is where the movie how <UNK> <UNK> from <UNK> comes on the way a fine emotional main \n",
      "<START> inspired by <UNK> <UNK> with this charm in least set up they gave for that best scenes of <UNK> \n",
      "<START> when i first saw with movies with a lot of a beautiful <UNK> as you hardly know that witch \n",
      "<START> oh dear oh dear <UNK> in its fat scene in control of a younger jones who was not <UNK> \n",
      "<START> i started watching this film that i couldn't see great horror films through the case of the cast also \n",
      "<START> a touching documentary that this isn't a decent why it is looking off here on an decent <UNK> as \n",
      "<START> let me first start to go to the movie <UNK> i think that people as maybe he was very \n",
      "<START> from 1996 first i had ever seen an special effects as it since i couldn't get about western obviously \n",
      "<START> ed <UNK> mitchell is his character good job of just many have ever she sees by the <UNK> <UNK> \n",
      "<START> i was really <UNK> br br in s a film that itself was <UNK> <START> this is a police \n",
      "<START> right so you have it with <UNK> for having much won't find you never know that it's important i \n"
     ]
    }
   ],
   "source": [
    "sample_sentences(num_sentences=20, sample_sent_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把weight正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_weights = np_utils.normalize(model.get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示意義相近單字的函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(word, nb_closest=10):\n",
    "    index = word_index[word]\n",
    "    distances = np.dot(norm_weights, norm_weights[index])\n",
    "    c_indexes = np.argsort(np.squeeze(distances))[-nb_closest:][::-1]\n",
    "    for c_index in c_indexes:\n",
    "        print(index_word[c_index], distances[c_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示意義相近的單字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== we\n",
      "we 1.0\n",
      "they 0.74794316\n",
      "you 0.6924391\n",
      "filmmakers 0.55942297\n",
      "you'll 0.52848184\n",
      "people 0.52046615\n",
      "makers 0.5174731\n",
      "you'd 0.5106005\n",
      "villains 0.5071005\n",
      "doesn't 0.50162345\n",
      "==== big\n",
      "big 1.0000001\n",
      "huge 0.64229536\n",
      "small 0.588405\n",
      "high 0.58337325\n",
      "major 0.51908267\n",
      "old 0.5152005\n",
      "our 0.5070703\n",
      "low 0.49837393\n",
      "sole 0.49291587\n",
      "upper 0.49155617\n",
      "==== great\n",
      "great 0.9999998\n",
      "decent 0.7066626\n",
      "good 0.7056813\n",
      "wonderful 0.68405986\n",
      "excellent 0.6526166\n",
      "fine 0.6511426\n",
      "terrific 0.62871826\n",
      "supporting 0.6087383\n",
      "outstanding 0.59913146\n",
      "stellar 0.58065677\n",
      "==== money\n",
      "money 1.0\n",
      "happen 0.5168859\n",
      "material 0.50648415\n",
      "sleep 0.48361927\n",
      "happened 0.4795894\n",
      "effort 0.47818303\n",
      "laugh 0.47398353\n",
      "sequel 0.46956882\n",
      "happens 0.46887144\n",
      "him 0.45606524\n",
      "==== years\n",
      "years 1.0\n",
      "year 0.6958463\n",
      "decades 0.54066837\n",
      "sit 0.5216108\n",
      "decade 0.503459\n",
      "months 0.47880787\n",
      "halfway 0.47178888\n",
      "weeks 0.458314\n",
      "sat 0.45318472\n",
      "days 0.4485622\n",
      "==== king\n",
      "king 1.0\n",
      "director 0.43279588\n",
      "master 0.42525703\n",
      "j 0.41008282\n",
      "sister 0.40408778\n",
      "daughter 0.3985269\n",
      "producer 0.39522666\n",
      "queen 0.39369428\n",
      "wife 0.38368046\n",
      "chief 0.3803403\n",
      "==== woman\n",
      "woman 0.99999994\n",
      "guy 0.73711574\n",
      "actress 0.6671771\n",
      "man 0.6623064\n",
      "girl 0.6468626\n",
      "doctor 0.612046\n",
      "cop 0.6004888\n",
      "dad 0.5902046\n",
      "actor 0.57679415\n",
      "person 0.5617722\n",
      "==== man\n",
      "man 1.0000001\n",
      "guy 0.7139708\n",
      "woman 0.6623064\n",
      "girl 0.6606432\n",
      "doctor 0.5985533\n",
      "person 0.5952902\n",
      "actress 0.5729108\n",
      "father 0.54902387\n",
      "cop 0.5445124\n",
      "boy 0.5317147\n",
      "==== book\n",
      "book 0.9999999\n",
      "ages 0.5805844\n",
      "script 0.5634302\n",
      "america 0.5303714\n",
      "house 0.5294546\n",
      "comedy 0.50746256\n",
      "ending 0.5059496\n",
      "situation 0.50525546\n",
      "flicks 0.5039039\n",
      "humor 0.5014279\n"
     ]
    }
   ],
   "source": [
    "words = [\"we\",\n",
    "         \"big\",\n",
    "         \"great\",\n",
    "         \"money\",\n",
    "         \"years\",\n",
    "         \"king\",\n",
    "         \"woman\",\n",
    "         \"man\",\n",
    "         \"book\",\n",
    "        ]\n",
    "\n",
    "for word in words:\n",
    "    if word in word_index:\n",
    "        print('====', word)\n",
    "        print_closest_words(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
